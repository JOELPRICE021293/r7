{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Marijse\n",
    "\n",
    "Aim: \n",
    "- Load the original data from the WR database from csv into a local postgres instance; \n",
    "- Load the different data views created from the original data from csv into a local postgres instance;\n",
    "\n",
    "Prerequisites: \n",
    "- Have all csv files loaded into the folder _6_data_clean & folder _10_data_views;\n",
    "- Have a local PSQL instance set up with a designated database for the rugby. Ensure that you know the password of your postgres user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2 in /Users/mvandenb/anaconda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: sqlalchemy in /Users/mvandenb/anaconda/lib/python3.6/site-packages\n"
     ]
    }
   ],
   "source": [
    "# sqlalchemy and psycopg2 might not be pre-installed, so we will run the installation of these packages first\n",
    "! pip install psycopg2\n",
    "! pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the necessary python packages to be able to run the code that follows\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update the below connection details based on your own psql connection details. \n",
    "# Note that the details are case sensitive. \n",
    "\n",
    "# Set up the database connection wiht Psycopg2. Adjust the dbname, user and password. \n",
    "db = psycopg2.connect(dbname='r7', user='postgres', password= 'postgres')\n",
    "cursor=db.cursor()\n",
    "\n",
    "# Set up a database connection using sqlalchemy. The first commented line explains what the different part of the \n",
    "# string stand for. Using this, adjust the user, password, port and dbname.  \n",
    "#engine = create_engine('postgres://user:password@localhost:port/database_name)\n",
    "engine = create_engine('postgres://postgres:postgres@localhost:5432/r7')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the psql database schema structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define variables\n",
    "# Define the input and output strings \n",
    "# The file path might need to be updated depending on where/how the files get stored. If you are working on a windows\n",
    "# system, comment out the below two lines and uncomment the lines for Windows underneath. \n",
    "input_string1 = '../_6_data_clean/'\n",
    "input_string2 = '../_10_data_views/'\n",
    "\n",
    "# Windows string :\n",
    "#input_string1 = '_6_data_clean\\\\'\n",
    "#input_string2 = '_10_data_views\\\\'\n",
    "\n",
    "# Define the schema names. We will create these in Postgres. For consistency and to enable us to share code easier, \n",
    "# please do not change thes. \n",
    "schema1 = '_0_original_data'\n",
    "schema2 = '_1_data_views'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the schemas in postgres. \n",
    "cursor.execute(\"CREATE SCHEMA IF NOT EXISTS \" + schema1)\n",
    "cursor.execute(\"CREATE SCHEMA IF NOT EXISTS \" + schema2)\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the different original csv files into psql (schema1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the files in the first input_string into the 'files' object. These files will go in the _0_original_data schema. \n",
    "files = glob.glob(input_string1 + \"**/*.csv\", recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_records_match_record_in_event\n",
      "team_dictionary\n",
      "team_records_in_event\n",
      "team_data_per_tournament\n",
      "player_records_match_record_in_event_all_time\n",
      "player_records_in_event\n",
      "team_match_records_in_event_all_time\n",
      "event_tables\n",
      "terminology_dictionary\n",
      "full_squads\n",
      "match_fixtures\n",
      "management\n",
      "team_record_in_event_all_time\n",
      "tournament_id_dictionary\n",
      "player_records_in_event_all_time\n",
      "all_time_head_to_head\n",
      "team_match_records_in_event\n"
     ]
    }
   ],
   "source": [
    "# we create an empty table for each file\n",
    "for k in files:\n",
    "    \n",
    "    # Set up the database connection wiht Psycopg2\n",
    "    db = psycopg2.connect(dbname='r7', user='postgres')\n",
    "    cursor=db.cursor()\n",
    "\n",
    "    # Set up a database connection using sqlalchemy\n",
    "    engine = create_engine('postgres://postgres:postgres@localhost:5432/r7')\n",
    "    \n",
    "    # We extract the tablename from the csv_file\n",
    "    table_name = k.split('/')[2].split('.')[0]\n",
    "    \n",
    "    #for windows only:\n",
    "    #table_name = k.split('\\\\')[1].split('.')[0]\n",
    "\n",
    "    print(table_name)\n",
    "    \n",
    "    # We load the csv into a df \n",
    "    df = pd.read_csv(k)\n",
    "    \n",
    "    # We clean up the column headers a bit\n",
    "    dict_columns = {}\n",
    "    for x in (df.columns.values):\n",
    "        dict_columns[x] = x.lower().replace(' ','_').replace(':','').replace('group','pool')\n",
    "    df = df.rename(columns=dict_columns)\n",
    "    \n",
    "    # We create an empty table with the table name\n",
    "    df.to_sql(schema=schema1, con=engine, if_exists='append', name=table_name, index=False)\n",
    "    \n",
    "    # We drop the column 'Unnamed: 0' if it exists\n",
    "    cursor.execute(\"alter table \"+ schema1 +\".\"+table_name+\" drop column if exists unnamed_0\",db)\n",
    "\n",
    "    db.commit()\n",
    "    db.close()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the different csv views on the data into psql (schema2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../_10_data_views/match_fixtures_features_row_per_team.csv',\n",
       " '../_10_data_views/fixtures_with_player_stats_full.csv',\n",
       " '../_10_data_views/player_stats_full.csv',\n",
       " '../_10_data_views/points_by_team_by_tournament.csv',\n",
       " '../_10_data_views/match_fixtures_full_row_per_match.csv',\n",
       " '../_10_data_views/team_data_with_timezone.csv']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the files in the second input_string into the 'files' object. These files will go in the _1_data_views schema. \n",
    "files = glob.glob(input_string2 + \"**/*.csv\", recursive=True)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match_fixtures_features_row_per_team\n",
      "fixtures_with_player_stats_full\n",
      "player_stats_full\n",
      "points_by_team_by_tournament\n",
      "match_fixtures_full_row_per_match\n",
      "team_data_with_timezone\n"
     ]
    }
   ],
   "source": [
    "# we create an empty table for each file\n",
    "for k in files:\n",
    "    \n",
    "    # Set up the database connection wiht Psycopg2\n",
    "    db = psycopg2.connect(dbname='r7', user='postgres')\n",
    "    cursor=db.cursor()\n",
    "\n",
    "    # Set up a database connection using sqlalchemy\n",
    "    engine = create_engine('postgres://postgres:postgres@localhost:5432/r7')\n",
    "    \n",
    "    # We extract the tablename from the csv_file\n",
    "    table_name = k.split('/')[2].split('.')[0]\n",
    "\n",
    "    #for windows only:\n",
    "    #table_name = k.split('\\\\')[1].split('.')[0]\n",
    "    \n",
    "    print(table_name)\n",
    "    \n",
    "    # We load the csv into a df \n",
    "    df = pd.read_csv(k)\n",
    "    \n",
    "    # We clean up the column headers a bit\n",
    "    dict_columns = {}\n",
    "    for x in (df.columns.values):\n",
    "        dict_columns[x] = x.lower().replace(' ','_').replace(':','').replace('group','pool')\n",
    "    df = df.rename(columns=dict_columns)\n",
    "    \n",
    "    # We create an empty table with the table name\n",
    "    df.to_sql(schema=schema2, con=engine, if_exists='append', name=table_name, index=False)\n",
    "    \n",
    "    # We drop the column 'Unnamed: 0' if it exists\n",
    "    cursor.execute(\"alter table \"+ schema2 +\".\"+table_name+\" drop column if exists unnamed_0\",db)\n",
    "\n",
    "    db.commit()\n",
    "    db.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
