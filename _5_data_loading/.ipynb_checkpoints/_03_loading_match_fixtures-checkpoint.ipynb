{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Author:__ Marijse\n",
    "\n",
    "__Aim:__ Load all match fixtures into a dataframe and send to Postgres. \n",
    "\n",
    "__Prerequisites:__\n",
    "- Have the tournament id dictionary loaded in postgres schema _0_original data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import urllib.request\n",
    "import requests\n",
    "import io\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up the database connection wiht Psycopg2\n",
    "db = psycopg2.connect(dbname='Rugby', user='postgres', host='localhost', password='password')\n",
    "cursor=db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up a database connection using sqlalchemy\n",
    "engine = create_engine('postgres://postgres:password@localhost:5432/Rugby')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating table and schema\n",
    "schema_name = '_0_original_data'\n",
    "cursor.execute(\"CREATE SCHEMA IF NOT EXISTS \" + schema_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Match Fixtures into datafrmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xml2df(xml_data):\n",
    "    root = ET.XML(xml_data) \n",
    "    all_records = []\n",
    "    for i, child in enumerate(root):\n",
    "        record = {}\n",
    "        for subchild in child:\n",
    "            record[subchild.tag] = subchild.text\n",
    "            all_records.append(record)\n",
    "    return pd.DataFrame(all_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1382', '1386', '1388', '1387', '1389', '1383', '1380', '1381', '1385', '1395', '1396', '1397', '1390', '1398', '1394', '1393', '1391', '1392', '1409', '1407', '1405', '1406', '1410', '1400', '1408', '1403', '1404', '1401', '1402', '1415', '1418', '1416', '1417', '1412', '1414', '1413', '1421', '1424', '1422', '1426', '1419', '1423', '1425', '1420', '1429', '1432', '897', '1433', '1428', '1430', '1436', '1440', '1437', '1441', '1434', '1439', '1438', '1435', '1445', '1447', '1446', '1449', '1443', '1448', '1444', '1450', '1453', '1455', '1454', '1457', '1451', '1456', '1452', '1458', '1462', '1464', '1463', '1466', '1459', '1465', '1460', '1467', '1470', '1472', '1471', '1475', '1473', '1468', '1469', '1474', '1479', '1481', '1480', '1484', '1477', '1482', '1478', '1483', '1491', '1488', '1492', '1489', '1490', '1485', '1487', '1493', '1486', '1500', '1497', '1521', '1501', '1498', '1522', '1499', '1494', '1496', '1523', '1502', '1495', '1524', '1509', '1527', '1506', '1510', '1508', '1507', '1529', '1530', '1505', '1504', '1528', '1319', '1320', '1321', '1543', '1559', '1544', '1545', '1546', '1560', '1547', '1561', '1548', '1549', '1562', '1550', '1563', '1551', '1564', '1629', '1611', '1612', '1613', '1614', '1630', '1615', '1616', '1617', '1631', '1618', '1632', '1619', '1620', '1640', '1719', '1684', '1685', '1686', '1720', '1687', '1688', '1748', '1689', '1690', '1691', '1721', '1692', '1693', '1722', '1723', '1809', '1786', '1787', '1788', '1810', '1789', '1790', '1791', '1792', '1811', '1793', '1812', '1795', '1813', '1794']\n"
     ]
    }
   ],
   "source": [
    "# List all event id's of the past two years\n",
    "eventid = pd.read_sql_query('select eventid from _0_original_data.tournament_id_dictionary',db)\n",
    "eventid_list = list(eventid.eventid.unique())\n",
    "print (eventid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define an empty dataframe to append the different xml files to\n",
    "full_fixtures = pd.DataFrame()\n",
    "\n",
    "for k in eventid_list:\n",
    "    url = 'http://webservices.irb.com/EventInformation.asmx/FixturesResults?uid=e9656db8-ffb5-4115-ac0b-cbd5688e6648&EventID=' + k\n",
    "    response = urllib.request.urlopen(url)\n",
    "    data = response.read()      \n",
    "    text = data.decode('utf-8') \n",
    "    \n",
    "    # Create a df which equals the text object\n",
    "    df = xml2df(text)\n",
    "    \n",
    "    # Skip all empty Event ID's\n",
    "    if df.empty == True:\n",
    "        continue\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # Extract the tournament id from the file name\n",
    "    tournament_id =  k\n",
    "        \n",
    "    # Set a column equal to the file name\n",
    "    df['tournament_id'] = tournament_id\n",
    "    \n",
    "    # Clean the column headers \n",
    "    dict_columns={}\n",
    "    for x in (df.columns.values):\n",
    "        dict_columns[x] = x.lower().replace('{http://webservices.irb.com/}','')\n",
    "    df_clean = df.rename(columns=dict_columns)\n",
    "    \n",
    "    # remove duplicates from the data\n",
    "    df_clean = df_clean.drop_duplicates(['matchid'], keep='first')\n",
    "    \n",
    "    # Append each individual dataframe to the full_fixtures df\n",
    "    frames =(full_fixtures,df_clean)\n",
    "    full_fixtures = pd.concat(frames)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8026"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_fixtures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract to CSV\n",
    "full_fixtures.to_csv('../_6_data_clean/match_fixtures.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract to SQL\n",
    "table_name = 'match_fixtures'\n",
    "full_fixtures.to_sql(schema=schema_name, con=engine, if_exists='replace', name=table_name)\n",
    "db.commit\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
