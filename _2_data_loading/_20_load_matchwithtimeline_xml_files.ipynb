{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as et\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import sqlalchemy as sa\n",
    "    \n",
    "res = []\n",
    "xml_data_dict = {}\n",
    "\n",
    "def compute_xml_data(lst,elem, func, level=0):\n",
    "    func(lst,elem,level)\n",
    "    for child in elem.getchildren():\n",
    "        compute_xml_data(lst,child, func, level+1)\n",
    "\n",
    "def gather_level(lst,elem,level):\n",
    "    lst.append(('-'*level+elem.tag, elem.text))\n",
    "\n",
    "xml_files = glob.glob('./*.xml')\n",
    "\n",
    "for xml_file in xml_files:\n",
    "    lst = []\n",
    "    root = et.parse(xml_file)\n",
    "    compute_xml_data(lst,root.getroot(), gather_level)\n",
    "    res.append(lst)\n",
    "\n",
    "def get_table_names(xml_data_list): \n",
    "    table_names = []\n",
    "    for t in xml_data_list:\n",
    "        if t[0]:\n",
    "            if re.match(r'(-{http://webservices.irb.com/}\\w)', t[0], re.IGNORECASE):\n",
    "                name = t[0][30:].strip()\n",
    "                table_names.append(name)\n",
    "    return table_names\n",
    "                \n",
    "tables = get_table_names(res[0])\n",
    "\n",
    "for xml_file in xml_files:\n",
    "    lst = []\n",
    "    root = et.parse(xml_file)\n",
    "    compute_xml_data(lst,root.getroot(), gather_level)\n",
    "\n",
    "# build all the dataframe from the xml, see next script for dataframe reference.\n",
    "def build_xml_dfs(xmls):\n",
    "    MatchID_tag = '--{http://webservices.irb.com/}MatchID'\n",
    "    match_info_tag = '-{http://webservices.irb.com/}MatchInformation'\n",
    "    officials_tag = '-{http://webservices.irb.com/}Officials'\n",
    "    official_tag = '--{http://webservices.irb.com/}Official'\n",
    "    team1TeamSheet_tag = '-{http://webservices.irb.com/}Team1TeamSheet'\n",
    "    team2TeamSheet_tag = '-{http://webservices.irb.com/}Team2TeamSheet'\n",
    "    player_tag = '--{http://webservices.irb.com/}Player'\n",
    "    timeline_tag = '-{http://webservices.irb.com/}Timeline'\n",
    "    timelineEntry_tag = '--{http://webservices.irb.com/}TimelineEntry'\n",
    "    \n",
    "    MatchID_idx = [xml[0] for xml in xmls].index(MatchID_tag)\n",
    "    MatchID = xmls[MatchID_idx][1]\n",
    "    \n",
    "    match_info_list = []\n",
    "    officials_list = []\n",
    "    team1TeamSheet_list = []\n",
    "    team2TeamSheet_list = []\n",
    "    timeline_list = []\n",
    "    \n",
    "    for t in xmls:\n",
    "        if t[0] == match_info_tag: \n",
    "            match_info_start_idx = [xml[0] for xml in xmls].index(match_info_tag)\n",
    "            officials_start_idx = [xml[0] for xml in xmls].index(officials_tag)\n",
    "            team1TeamSheet_start_idx = [xml[0] for xml in xmls].index(team1TeamSheet_tag)\n",
    "            team2TeamSheet_start_idx = [xml[0] for xml in xmls].index(team2TeamSheet_tag)\n",
    "            timeline_start_idx = [xml[0] for xml in xmls].index(timeline_tag)\n",
    "\n",
    "            for m in xmls[match_info_start_idx+1:officials_start_idx]:\n",
    "                if m[0] is not officials_tag:\n",
    "                    match_info_list.append((m[0][31:].strip(),m[1]))\n",
    "                else:\n",
    "                    continue\n",
    "            for o in xmls[officials_start_idx+1:team1TeamSheet_start_idx]:\n",
    "                if o[0] == official_tag:\n",
    "                    sub_list = []\n",
    "                    sub_list.append(('MatchID',MatchID))\n",
    "                    officials_list.append(sub_list)\n",
    "                elif o[0] is not team1TeamSheet_tag:\n",
    "                    sub_list.append((o[0][32:].strip(),o[1]))\n",
    "                else:\n",
    "                    continue\n",
    "            for t1 in xmls[team1TeamSheet_start_idx+1:team2TeamSheet_start_idx]: #25/132\n",
    "                if t1[0] == player_tag:\n",
    "                    sub_list = []\n",
    "                    sub_list.append(('MatchID',MatchID))\n",
    "                    team1TeamSheet_list.append(sub_list)\n",
    "                elif t1[0] is not team2TeamSheet_tag:\n",
    "                    sub_list.append((t1[0][32:].strip(),t1[1]))\n",
    "                else:\n",
    "                    continue\n",
    "            for t2 in xmls[team2TeamSheet_start_idx+1:timeline_start_idx]:\n",
    "                if t2[0] == player_tag:\n",
    "                    sub_list = []\n",
    "                    sub_list.append(('MatchID',MatchID))\n",
    "                    team2TeamSheet_list.append(sub_list)\n",
    "                elif t2[0] is not team2TeamSheet_tag:\n",
    "                    sub_list.append((t2[0][32:].strip(),t2[1]))\n",
    "                else:\n",
    "                    continue\n",
    "            for time_line in xmls[timeline_start_idx+1:]:\n",
    "                if time_line[0] == timelineEntry_tag:\n",
    "                    sub_list = []\n",
    "                    sub_list.append(('MatchID',MatchID))\n",
    "                    timeline_list.append(sub_list)\n",
    "                elif time_line[0]:\n",
    "                    sub_list.append((time_line[0][32:].strip(),time_line[1]))\n",
    "                else:\n",
    "                    break                 \n",
    "\n",
    "    records_dict = {'match_info': match_info_list, 'officials': officials_list, 'team1TeamSheet': team1TeamSheet_list, 'team2TeamSheet':team2TeamSheet_list,'timeline':timeline_list}\n",
    "    \n",
    "    match_info_df = pd.DataFrame.from_records(match_info_list).transpose()\n",
    "    match_info_df = match_info_df.rename(columns=match_info_df.iloc[0]).drop(match_info_df.index[0])\n",
    "    \n",
    "    officials_df = [pd.DataFrame.from_records(lst).transpose() for lst in officials_list]\n",
    "    officials_df = [df.rename(columns=df.iloc[0]).drop(df.index[0]) for df in officials_df]\n",
    "    \n",
    "    team1TeamSheet_df = [pd.DataFrame.from_records(lst).transpose() for lst in team1TeamSheet_list]\n",
    "    team1TeamSheet_df = [df.rename(columns=df.iloc[0]).drop(df.index[0]) for df in team1TeamSheet_df]\n",
    "    \n",
    "    team2TeamSheet_df = [pd.DataFrame.from_records(lst).transpose() for lst in team2TeamSheet_list]\n",
    "    team2TeamSheet_df = [df.rename(columns=df.iloc[0]).drop(df.index[0]) for df in team2TeamSheet_df]\n",
    "    \n",
    "    timeline_df = [pd.DataFrame.from_records(lst).transpose() for lst in timeline_list]\n",
    "    timeline_df = [df.rename(columns=df.iloc[0]).drop(df.index[0]) for df in timeline_df]\n",
    "    \n",
    "    match_info_dfs_merged = match_info_df\n",
    "    officials_dfs_merged = pd.concat(officials_df)\n",
    "    team1TeamSheet_dfs_merged = pd.concat(team1TeamSheet_df)\n",
    "    team2TeamSheet_dfs_merged = pd.concat(team2TeamSheet_df)\n",
    "    timeline_dfs_merged = pd.concat(timeline_df)\n",
    "    \n",
    "    return {'match_info':match_info_dfs_merged,\n",
    "            'officials':officials_dfs_merged,\n",
    "            'team1TeamSheet':team1TeamSheet_dfs_merged,\n",
    "            'team2TeamSheet':team2TeamSheet_dfs_merged,\n",
    "            'timeline':timeline_dfs_merged}\n",
    "\n",
    "all_xml_dfs = []\n",
    "match_info_dfs_list = []\n",
    "officials_dfs_list = []\n",
    "team1TeamSheet_dfs_list = []\n",
    "team2TeamSheet_dfs_list = []\n",
    "timeline_dfs_list = []\n",
    "\n",
    "for xml_data in res:\n",
    "    all_xml_dfs.append(build_xml_dfs(xml_data))\n",
    "\n",
    "for df_dict in all_xml_dfs:\n",
    "    for k,v in df_dict.items():\n",
    "        if k == 'match_info':\n",
    "            match_info_dfs_list.append(v)\n",
    "        if k == 'officials':\n",
    "            officials_dfs_list.append(v)\n",
    "        if k == 'team1TeamSheet':\n",
    "            team1TeamSheet_dfs_list.append(v)\n",
    "        if k == 'team2TeamSheet':\n",
    "            team2TeamSheet_dfs_list.append(v)\n",
    "        if k == 'timeline':\n",
    "            timeline_dfs_list.append(v)\n",
    "\n",
    "match_info_dfs_merged_all = pd.concat(match_info_dfs_list)\n",
    "officials_dfs_merged_all = pd.concat(officials_dfs_list)\n",
    "team1TeamSheet_dfs_merged_all = pd.concat(team1TeamSheet_dfs_list)\n",
    "team2TeamSheet_dfs_merged_all = pd.concat(team2TeamSheet_dfs_list)\n",
    "timeline_dfs_merged_all = pd.concat(timeline_dfs_list)\n",
    "\n",
    "# load dataframe into Postgres\n",
    "def write_data_to_sql(df, table_name):        \n",
    "    try:\n",
    "        engine = sa.create_engine(connection_string, echo=True);\n",
    "        # add a table id?\n",
    "        # df[\"TableId\"] = tableId;\n",
    "        if not df.empty:\n",
    "            df.to_sql(tablename, engine, if_exists='append', index=False)       \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "match_info_dfs_merged_all.to_csv('../_3_data_clean/matchwithtimeline_matchinformation.csv',index=False)\n",
    "officials_dfs_merged_all.to_csv('../_3_data_clean/matchwithtimeline_officials.csv',index=False)\n",
    "team1TeamSheet_dfs_merged_all.to_csv('../_3_data_clean/matchwithtimeline_team1teamsheet.csv',index=False)\n",
    "team2TeamSheet_dfs_merged_all.to_csv('../_3_data_clean/matchwithtimeline_team2teamsheet.csv',index=False)\n",
    "timeline_dfs_merged_all.to_csv('../_3_data_clean/matchwithtimeline_timeline.csv',index=False)\n",
    "print('Completed!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run below script to find out the xml format, only run if format is due to change. \n",
    "import xml.etree.ElementTree as et\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import sqlalchemy as sa\n",
    "    \n",
    "res = []\n",
    "\n",
    "def compute_xml_data(elem, func, level=0):\n",
    "    func(elem,level)\n",
    "    for child in elem.getchildren():\n",
    "        compute_xml_data(child, func, level+1)\n",
    "\n",
    "def gather_level(elem,level):\n",
    "    print('-'*level+elem.tag, elem.text)\n",
    "#     res.append(('-'*level+elem.tag, elem.text))\n",
    "\n",
    "xml_files = glob.glob('./*.xml')\n",
    "\n",
    "\n",
    "xml_data = compute_xml_data(root.getroot(), gather_level)\n",
    "\n",
    "def get_table_names(xml_data_list): \n",
    "    table_names = []\n",
    "    for t in xml_data_list:\n",
    "        if t[0]:\n",
    "            if re.match(r'(-{http://webservices.irb.com/}\\w)', t[0], re.IGNORECASE):\n",
    "                name = t[0][30:].strip()\n",
    "                table_names.append(name)\n",
    "    return table_names\n",
    "\n",
    "# print(res)\n",
    "\n",
    "print(get_table_names(res))\n",
    "\n",
    "# compute_xml_data(root.getroot(), gather_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
